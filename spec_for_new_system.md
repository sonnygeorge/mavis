## Motivation

- One interesting flavor of scene compositionality is how actions compose with their role arguments.
- In this regard, T2I models are known to struggle with:
  1. Asymmetric bias in role assignments
  2. 3-or-more-argument action compositions (e.g., ditransitive verbs) with atypical arguments
- We propose a semi-procedural generation pipeline to controllably generate images of such scenes.

## Pipeline Input

(specified by a human in YAML format)

```yaml
action_scene: A puma shows a bird to a person. A dog is nearby.
action: shows
role_assignments:
  shower: puma
  shown: bird
  shown_to: person
  nearby: dog
localized_edits:
  - region_contains:
      - shower
      - shown
    requirements:
      - "{shower}'s body is oriented towards {shown_to}."
      - "{shower}'s gaze is looking at {shown}."
      - "{shower}'s arm is outstretched."
      - "{shown} is perched on {shower}'s arm."
  - region_contains:
      - shown_to
    requirements:
      - "{shown_to} is close to {shower} with their body oriented towards them."
      - "{shown_to}'s gaze is looking at {shown}."
  - region_contains:
      - nearby
    requirements:
      - "{nearby} is in the scene periphery, paying no attention to {shower}, {shown}, or {shown_to}."
```

```yaml
action_scene: A person throws a laptop over a sign to a dog.
action: throws
role_assignments:
  thrower: person
  thrown: laptop
  thrown_to: dog
  thrown_over: sign
localized_edits:
  - region_contains:
      - thrown_over
    requirements:
      - "{thrown_over} is directly in between {thrower} and {thrown_to}."
  - region_contains:
      - thrown
    requirements:
      - "{thrown} is airborne directly above {thrown_over}."
      - "wind marks trail behind {thrown}, indicating it is flying from {thrower} to {thrown_to}."
  - region_contains:
      - thrower
    requirements:
      - "{thrower}'s body is in the terminal position/follow-through of having thrown {thrown}."
  - region_contains:
      - thrown_to
    requirements:
      - "{thrown_to}'s gaze is looking up at {thrown}, anticipating its arrival."
```

## Pipeline Run Object

```python
class Hyperparameters:
    num_blender_scenes: int # $n_s$
    num_renders: int # $n_p$
    min_render_quality_score: float # $t_r$
    max_edit_attempts: int
    model_selection_probs_by_edit_attempt_ranges: ...

@dataclass
class BlenderSceneParams:
    # Generated by LM based off visual requirements across localized edits
    ...

@dataclass
class EditStep:
    before_img_path: Path
    after_img_path: Path

@dataclass
class BlenderSceneRender:
    scene_params: BlenderSceneParams
    path: Path
    edits: list[EditStep]

@dataclass
class PipelineRun:
    scene_spec: ...
    hyperparameters: Hyperparameters
    renders: list[BlenderSceneRender]
```

## Step 1: Generate Blender Scene Renders

1. Generate $n_s$ blender scene params
2. For each:
    - Setup a blender scene
    - Render $n_p$ povs, calculating segment boundaries for each edit region
3. Get `render_quality_scores` for all renders
4. Select top $k$ renders from blender scene w/ highest mean `render_quality_score`
5. Error out if mean `render_quality_score` of these top $k$ renders is less than $t_r$

## Step 2: Apply Localized Edits To Get Final Images

```python
@dataclass
class YesNoAnswer:
    answer: Literal["yes", "no"]
    confidence: float  # 0.0 (no confidence) to 1.0 (full confidence)
```

1. For each render:
    - for i in `max_edit_attempts`:
        - Do background edit
        - Perform background edit requirement checks and as soon as one fails, either continue to next edit attempt or, if on last attempt, mark render as failure and abandon editing it further.
        - If all checks pass, break.
    - For each localized edit:
        - Do this edit's requirements pass? If yes, continue (no need to apply edit)
        - For i in `max_edit_attempts`:
            - Apply edit (using drawn-on segmentation boundary?)
            - Loop through accumulating requirement checks and as soon as one fails, either continue to next edit attempt or, if on last attempt, mark render as failure and abandon editing it.
            - If all checks pass, break and continue to next edit.
    - If all edits successful, save last edited image as final image.

NOTE: Even higher avg. quality can be achieved by ranking/dropping some % from final set of images based on some aggregation of confidence score across last round of checks.

## Design Questions

- How to parallelize?
- How to maintain traces and visualize asynchronously growing trees of renders/edits in GUI?